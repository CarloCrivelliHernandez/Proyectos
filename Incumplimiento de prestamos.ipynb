{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incumplimiento de prestamos\n",
    "* Carlo Crivelli Hernández"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se analizo y limpio la base de datos de un banco con el proposito de saber que caracteristicas tienen las personas que no cumplen con los pagos de los presta,os bancarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import colors as mcolors\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analisis_Predictivo:\n",
    "    def __init__(self,datos:DataFrame, predecir:str, predictoras:list = [],\n",
    "                 modelo = None,train_size:int = 80,random_state:int = None):\n",
    "        '''\n",
    "        Ajusta un modelo basado en sci-kit learn para realizar predicciones sobre prestatarios.\n",
    "\n",
    "        datos: Datos completos y listos para construir un modelo\n",
    "        \n",
    "        modelo: Instancia de una Clase de un método de clasificación(KNN,Árboles,SVM,etc).\n",
    "        Si no especifica un modelo no podrá utilizar el método fit_n_review()\n",
    "        \n",
    "        predecir: Nombre de la variable a predecir\n",
    "        \n",
    "        predictoras: Lista de los nombres de las variables predictoras.\n",
    "        Si vacío entonces utiliza todas las variables presentes excepto la variable a predecir.\n",
    "        \n",
    "        train_size: Proporción de la tabla de entrenamiento respecto a la original.\n",
    "        \n",
    "        random_state: Semilla aleatoria para la división de datos(training-testing).\n",
    "        '''        \n",
    "        self.datos = datos\n",
    "        self.predecir = predecir\n",
    "        self.predictoras = predictoras\n",
    "        self.modelo = modelo\n",
    "        self.random_state = random_state\n",
    "        if modelo != None:\n",
    "            self.train_size = train_size\n",
    "            self._training_testing()\n",
    "        \n",
    "        \n",
    "    def _training_testing(self):\n",
    "        if len(self.predictoras) == 0:\n",
    "            X = self.datos.drop(columns=[self.predecir])\n",
    "        else:\n",
    "            X = self.datos[self.predictoras]\n",
    "            \n",
    "        y = self.datos[self.predecir].values\n",
    "        \n",
    "        train_test = train_test_split(X, y, train_size=self.train_size, \n",
    "                                      random_state=self.random_state)\n",
    "        self.X_train, self.X_test,self.y_train, self.y_test = train_test\n",
    "        \n",
    "        \n",
    "    def fit_predict(self):\n",
    "        if(self.modelo != None):\n",
    "            self.modelo.fit(self.X_train,self.y_train)\n",
    "            return self.modelo.predict(self.X_test)\n",
    "        \n",
    "    def fit_predict_resultados(self, imprimir = True):\n",
    "        if(self.modelo != None):\n",
    "            y = self.datos[self.predecir].values\n",
    "            prediccion = self.fit_predict()\n",
    "            MC = confusion_matrix(self.y_test, prediccion)\n",
    "            indices = self.indices_general(MC,list(np.unique(y)))\n",
    "            if imprimir == True:\n",
    "                for k in indices:\n",
    "                    print(\"\\n%s:\\n%s\"%(k,str(indices[k])))\n",
    "            \n",
    "            # return indices\n",
    "            return MC\n",
    "    \n",
    "    def indices_general(self,MC, nombres = None):\n",
    "        \"Método para calcular los índices de calidad de la predicción\"\n",
    "        precision_global = np.sum(MC.diagonal()) / np.sum(MC)\n",
    "        error_global = 1 - precision_global\n",
    "        precision_categoria  = pd.DataFrame(MC.diagonal()/np.sum(MC,axis = 1)).T\n",
    "        if nombres!=None:\n",
    "            precision_categoria.columns = nombres\n",
    "        return {\"Matriz de Confusión\":MC, \n",
    "                \"Precisión Global\":precision_global, \n",
    "                \"Error Global\":error_global, \n",
    "                \"Precisión por categoría\":precision_categoria}\n",
    "    \n",
    "    def distribucion_variable_predecir(self):\n",
    "        \"Método para graficar la distribución de la variable a predecir\"\n",
    "        variable_predict = self.predecir\n",
    "        data = self.datos\n",
    "        colors = list(dict(**mcolors.CSS4_COLORS))\n",
    "        df = pd.crosstab(index=data[variable_predict],columns=\"valor\") / data[variable_predict].count()\n",
    "        fig = plt.figure(figsize=(10,2))\n",
    "        g = fig.add_subplot(111)\n",
    "        countv = 0\n",
    "        titulo = \"Distribución de la variable %s\" % variable_predict\n",
    "        for i in range(df.shape[0]):\n",
    "            g.barh(1,df.iloc[i],left = countv, align='center',color=colors[11+i],label= df.iloc[i].name)\n",
    "            countv = countv + df.iloc[i]\n",
    "        vals = g.get_xticks()\n",
    "        g.set_xlim(0,1)\n",
    "        g.set_yticklabels(\"\")\n",
    "        g.set_title(titulo)\n",
    "        g.set_ylabel(variable_predict)\n",
    "        g.set_xticklabels(['{:.0%}'.format(x) for x in vals])\n",
    "        countv = 0 \n",
    "        for v in df.iloc[:,0]:\n",
    "            g.text(np.mean([countv,countv+v]) - 0.03, 1 , '{:.1%}'.format(v), color='black', fontweight='bold')\n",
    "            countv = countv + v\n",
    "        g.legend(loc='upper center', bbox_to_anchor=(1.08, 1), shadow=True, ncol=1)\n",
    "        \n",
    "    def poder_predictivo_categorica(self, var:str):\n",
    "        \"Método para ver la distribución de una variable categórica respecto a la predecir\"\n",
    "        data = self.datos\n",
    "        variable_predict = self.predecir\n",
    "        df = pd.crosstab(index= data[var],columns=data[variable_predict])\n",
    "        df = df.div(df.sum(axis=1),axis=0)\n",
    "        titulo = \"Distribución de la variable %s según la variable %s\" % (var,variable_predict)\n",
    "        g = df.plot(kind='barh',stacked=True,legend = True, figsize = (10,9), \\\n",
    "                    xlim = (0,1),title = titulo, width = 0.8)\n",
    "        vals = g.get_xticks()\n",
    "        g.set_xticklabels(['{:.0%}'.format(x) for x in vals])\n",
    "        g.legend(loc='upper center', bbox_to_anchor=(1.08, 1), shadow=True, ncol=1)\n",
    "        for bars in g.containers:\n",
    "            plt.setp(bars, width=.9)\n",
    "        for i in range(df.shape[0]):\n",
    "            countv = 0 \n",
    "            for v in df.iloc[i]:\n",
    "                g.text(np.mean([countv,countv+v]) - 0.03, i , '{:.1%}'.format(v), color='black', fontweight='bold')\n",
    "                countv = countv + v\n",
    "                \n",
    "                \n",
    "    def poder_predictivo_numerica(self,var:str):\n",
    "        \"Función para ver la distribución de una variable numérica respecto a la predecir\"\n",
    "        sns.FacetGrid(self.datos, hue=self.predecir, height=6).map(sns.kdeplot, var, shade=True).add_legend()\n",
    "\n",
    "    def predictNew(self,df):\n",
    "        '''\n",
    "        Realiza nuevas predicciones con un dataframe nuevo con las mismas caracteristicas del de entrenamiento\n",
    "        '''\n",
    "        self.X_test=df.drop(self.predecir,axis=1)\n",
    "        self.y_test=df[self.predecir]\n",
    "        return self.fit_predict_resultados()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar la base de datos\n",
    "bd_prestamos = pd.read_csv('Loan_Default.csv', index_col=0)\n",
    "bd_prestamos.head(10)\n",
    "bd_prestamos=bd_prestamos.dropna(subset=['Status'],axis=0)\n",
    "\n",
    "# alternativa pa quitar nans\n",
    "count_nan_all = bd_prestamos.isna().sum()\n",
    "print('Se quitaron las siguientes columnas por datos insuficientes:\\n'+'-'*60)\n",
    "for idx, val in enumerate(bd_prestamos.columns):\n",
    "    if count_nan_all[val]>20_000 and bd_prestamos[val].dtype in ('float64', 'int', 'float'):\n",
    "        bd_prestamos = bd_prestamos.drop(val, axis=1)\n",
    "        print(f'{val:38}',f'{count_nan_all[val]} datos faltantes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAnalizaremos las siguientes variables:\\n'+'-'*63)\n",
    "pending=[]\n",
    "count_nan_all = bd_prestamos.isna().sum()\n",
    "for idx, val in enumerate(bd_prestamos.columns):\n",
    "    if count_nan_all[val] and bd_prestamos[val].dtype in ('float64', 'int', 'float'):\n",
    "        print(f'{val:38}',f'{count_nan_all[val]:8} datos faltantes')\n",
    "        pending.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, quitaremos los valores faltantes de la columna 'term' ya que son muy pocos para proporcionarnos algo de informacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_prestamos=bd_prestamos.dropna(subset=['term'],axis=0)\n",
    "if 'term' in pending:pending.remove('term')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitamos los outliers de la muestra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_inplace(df, columns, z_score_threshold=3):\n",
    "    \"\"\"\n",
    "    Removes outliers for specified columns in the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        - df: The DataFrame to modify in place.\n",
    "        - columns: A list of column names for which to remove outliers.\n",
    "        - z_score_threshold: The threshold for identifying outliers based on z-scores.\n",
    "\n",
    "    Returns:\n",
    "        None (modifies the original DataFrame in place).\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        # Check if the specified column exists in the DataFrame\n",
    "        if col in df.columns:\n",
    "            # Calculate the z-scores for non-NaN values in the column\n",
    "            non_nan_values = df[col].notna()\n",
    "            z_scores = np.abs((df.loc[non_nan_values, col] - df.loc[non_nan_values, col].mean()) / df.loc[non_nan_values, col].std())\n",
    "\n",
    "            # Identify rows with z-scores exceeding the threshold\n",
    "            outliers = df.index[non_nan_values][z_scores > z_score_threshold]\n",
    "\n",
    "            # Remove rows with outliers in place\n",
    "            df.drop(outliers, inplace=True)\n",
    "\n",
    "# Example usage:\n",
    "# df is your original DataFrame, which may contain NaN values\n",
    "# columns_to_remove_outliers is a list of column names where you want to remove outliers\n",
    "# The function will modify the original DataFrame in-place for the specified columns.\n",
    "columns_to_remove_outliers = pending\n",
    "remove_outliers_inplace(bd_prestamos, columns_to_remove_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el resto de valores en intervalos optimos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_numeric_values_to_intervals_inplace(df, columns):\n",
    "    \"\"\"\n",
    "    Splits numeric values in specified columns into intervals for the original DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        - df: The DataFrame to modify in place.\n",
    "        - columns: A list of column names for which to split numeric values into intervals.\n",
    "\n",
    "    Returns:\n",
    "        None (modifies the original DataFrame in place).\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        # Check if the specified column exists in the DataFrame\n",
    "        if col in df.columns:\n",
    "            # Select non-NaN values in the column\n",
    "            non_nan_values = df[col].notna()\n",
    "            values = df.loc[non_nan_values, col]\n",
    "\n",
    "            # Calculate the number of intervals based on the values using the Freedman-Diaconis rule\n",
    "            iqr = np.percentile(values, 75) - np.percentile(values, 25)\n",
    "            bin_width = 2.0 * iqr / np.power(len(values), 1/3)\n",
    "            num_intervals = int(np.ceil((values.max() - values.min()) / bin_width))\n",
    "\n",
    "            # Create intervals and assign them to the DataFrame\n",
    "            intervals = pd.cut(values, bins=num_intervals, precision=2)\n",
    "            df.loc[non_nan_values, col] = intervals.astype(str)\n",
    "columns_to_modify = pending\n",
    "split_numeric_values_to_intervals_inplace(bd_prestamos, columns_to_modify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los valores faltantes en una nueva clase llamada \"No disponible\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_prestamos.fillna('No disponible')\n",
    "print('Valores unicos por columna:\\n'+'-'*30)\n",
    "for col in pending:\n",
    "    print(f'{col:25}',bd_prestamos[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapeamos los valores unicos de las variables categoricas, incluyendo las que acabamos de cambiar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(bd_prestamos.columns)[0:-1]:\n",
    "  unqvals = []\n",
    "  if bd_prestamos[col].nunique()>160: continue\n",
    "  for unqval in bd_prestamos[col].unique():\n",
    "      unqvals.append(unqval)\n",
    "  unique_vals_map = {unq:i for i,unq in enumerate(unqvals)}\n",
    "  if unique_vals_map:\n",
    "    bd_prestamos[col] = bd_prestamos[col].map(unique_vals_map)\n",
    "\n",
    "count_nan_all = bd_prestamos.isna().sum()\n",
    "nada=''\n",
    "print(f'Variable{nada:20}Valores perdidos\\n'+'-'*45)\n",
    "for idx, val in enumerate(bd_prestamos.columns):\n",
    "    print(f'{val:35}{count_nan_all[val]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calcularemos las correlaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataplot = sns.heatmap(bd_prestamos.corr(), cmap= sns.cubehelix_palette(as_cmap = True ),\n",
    "                       annot= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que hay variables con 100% de correlacion. Veremos cuales son\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = bd_prestamos.corr()\n",
    "\n",
    "# Find columns with correlation >= 0.9\n",
    "highly_correlated_cols = []\n",
    "for col1 in correlation_matrix.columns:\n",
    "    for col2 in correlation_matrix.columns:\n",
    "        if col1 != col2 and abs(correlation_matrix[col1][col2]) >= 0.9:\n",
    "            highly_correlated_cols.append((col1, col2))\n",
    "\n",
    "# Print highly correlated columns\n",
    "nada=''\n",
    "print(f'Variable 1{nada:20}Variable 2{nada:20}Correlacion')\n",
    "for col1, col2 in highly_correlated_cols:\n",
    "\n",
    "    print(f\"{col1:30}{col2:32} {correlation_matrix[col1][col2]:.2f}\")\n",
    "\n",
    "bd_prestamos=bd_prestamos.drop(['construction_type','Security_Type'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cuantificaremos la informacion que cada variable nos porporciona para una prediccion y quitaremos las que menos nos sirvan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "info_by_var=list()\n",
    "vars_out=list()\n",
    "quedan=['Gender','Region','age','income']\n",
    "def entropy_based_predictive_power(df: pd.DataFrame, target: str, predictor: str) -> float:\n",
    "    # Calculate the mutual information between the predictor and target variables\n",
    "    mi = mutual_info_score(df[target], df[predictor])\n",
    "    return mi\n",
    "nada=''\n",
    "print(f'Variable{nada:20}Medida de poder predictivo (entropia){nada:20}Suficiente')\n",
    "for col in list(bd_prestamos.columns)[0:-1]:\n",
    "    mi=entropy_based_predictive_power(bd_prestamos[[col,'Status']],'Status',col)\n",
    "    txt='Si' if mi>.0009 else 'No'\n",
    "    if mi<.0009 and col not in quedan: vars_out.append(col)\n",
    "    print(f'{col:45}{mi:.5f}{nada:37}{txt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la nueva base de datos con nuestros predictores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quitar columnas que no sirven\n",
    "bd_predictiva = bd_prestamos.drop(vars_out, axis=1)\n",
    "bd_predictiva = bd_predictiva.drop_duplicates()\n",
    "\n",
    "bd_predictiva\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui hacemos analisis de factores y agarramos los mas importantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "def factor_analysis(df: pd.DataFrame, n_factors: int = 15, rotation: str = 'varimax', method: str = 'lda') -> None:\n",
    "    '''\n",
    "    - df: Un dataframe con variables correlacionadas\n",
    "    - n_factors: Numero de factores obtenidos a partir de las variables\n",
    "    - method: alguno de los siguientes ['ml', 'mle', 'uls', 'minres', 'principal']\n",
    "\n",
    "    '''\n",
    "    fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation, method=method)\n",
    "    fa.fit(df)\n",
    "    #Metodos ['ml', 'mle', 'uls', 'minres', 'principal']\n",
    "\n",
    "    # Obtener cargas\n",
    "    loadings = fa.loadings_\n",
    "\n",
    "    # Obtener varianzas\n",
    "    variance = fa.get_factor_variance()\n",
    "\n",
    "    # Obtener comunalidades\n",
    "    communalities = fa.get_communalities()\n",
    "\n",
    "    # Formatear Output\n",
    "    loadings_table = 'Cargas Factoriales:\\n'\n",
    "    loadings_table += 'Variable\\t' + '\\t'.join([f'Factor {i+1}' for i in range(n_factors)]) + '\\tFactor mas repr.'+'\\tComunalidad\\n'\n",
    "    for variable, loading,communality in zip(df.columns, loadings,communalities):\n",
    "        most_representative_factor = np.argmax(loading) + 1\n",
    "        commie = communality\n",
    "        loadings_table += f'{variable:<15}\\t' + '\\t'.join([f'{loading[i]:<10.2f}' for i in range(n_factors)]) + f'\\tF{most_representative_factor} : {loading[np.argmax(loading)]:.2f}'+f'\\t{commie:>23.2f}\\n'\n",
    "\n",
    "    a=''\n",
    "    variance_table = 'Varianza explicada por cada factor:\\n'\n",
    "    variance_table += f'Varianza{a:>13}\\t' + '\\t'.join([f'{variance[0][i]:<10.2f}' for i in range(n_factors)]) + f'Tot: {sum([variance[0][i] for i in range(n_factors)]):.3f}\\n'\n",
    "    variance_table += f'%Var{a:>18}\\t' + '\\t'.join([f'{variance[1][i]:<10.2f}' for i in range(n_factors)]) + f'Tot: {sum([variance[1][i] for i in range(n_factors)]):.3f}\\n'\n",
    "    variance_table += f'%Var. cumulativa\\t' + '\\t'.join([f'{variance[2][i]:<10.3f}' for i in range(n_factors)]) + '\\n'\n",
    "\n",
    "    print(loadings_table)\n",
    "    print(variance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_analysis(bd_predictiva.drop('Status',axis=1),n_factors= 12,rotation='varimax',method='principal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducimos las dimensiones del dataste para eliminar ruido en nuestras variables predictoras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def reduce_dimensions(dataframe:pd.DataFrame, target_variable:str, method:str='PCA', num_components=None, num_clusters=None):\n",
    "\n",
    "# Separate predictor variables and target variable\n",
    "    X = dataframe.drop(target_variable, axis=1)\n",
    "    y = dataframe[target_variable]\n",
    "\n",
    "    if method == 'PCA':\n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=num_components)\n",
    "        reduced_data = pca.fit_transform(X)\n",
    "\n",
    "    elif method == 'KMeans':\n",
    "        # Perform K-Means clustering\n",
    "        kmeans = KMeans(n_clusters=num_clusters)\n",
    "        reduced_data = kmeans.fit_transform(X)\n",
    "\n",
    "    elif method == 'FactorAnalysis':\n",
    "        # Perform Factor Analysis\n",
    "        fa = FactorAnalyzer(n_factors=num_components, rotation='varimax')\n",
    "        fa.fit(X)\n",
    "        reduced_data = fa.transform(X)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'PCA', 'KMeans', or 'FactorAnalysis'.\")\n",
    "\n",
    "    # Create a DataFrame with reduced dimensions\n",
    "    reduced_df = pd.DataFrame(reduced_data, columns=[f'Dimension_{i + 1}' for i in range(reduced_data.shape[1])])\n",
    "\n",
    "    # Add the target variable back to the DataFrame\n",
    "    reduced_df[target_variable] = y\n",
    "\n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para determinar los mejores parametros del modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "scoringLDA = Analisis_Predictivo(bd_predictiva, predecir=\"Status\", modelo=lda,\n",
    "                                       train_size=0.8, random_state=0)\n",
    "scoringLDA.distribucion_variable_predecir()\n",
    "plt.show()\n",
    "resLDA = scoringLDA.fit_predict_resultados()\n",
    "sns.heatmap(resLDA, annot=True, fmt='3.0f', xticklabels=('Falsos Positivos','Verdaderos Positivos'), \n",
    "            yticklabels=('Verdaderos Positivos','Falsos Positivos'), cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Matriz de Confusion (LDA)', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "scoringQDA = Analisis_Predictivo(bd_predictiva, predecir=\"Status\", modelo=qda,\n",
    "                                       train_size=0.8, random_state=0)\n",
    "scoringQDA.distribucion_variable_predecir()\n",
    "plt.show()\n",
    "resQDA = scoringQDA.fit_predict_resultados()\n",
    "sns.heatmap(resQDA, annot=True, fmt='3.0f', xticklabels=('Falsos Positivos','Verdaderos Positivos'), \n",
    "            yticklabels=('Verdaderos Positivos','Falsos Positivos'), cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Matriz de Confusion (QDA)', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bys = GaussianNB()\n",
    "scoringBYS = Analisis_Predictivo(bd_predictiva, predecir=\"Status\", modelo=bys,\n",
    "                                       train_size=0.8, random_state=0)\n",
    "scoringBYS.distribucion_variable_predecir()\n",
    "plt.show()\n",
    "resBYS = scoringBYS.fit_predict_resultados()\n",
    "sns.heatmap(resBYS, annot=True, fmt='3.0f', xticklabels=('Falsos Positivos','Verdaderos Positivos'), \n",
    "            yticklabels=('Verdaderos Positivos','Falsos Positivos'), cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Matriz de Confusion (Naive Bayes)', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "bys.fit(scoringBYS.X_train, scoringBYS.y_train)\n",
    "\n",
    "r_probs = [0 for _ in range(len(scoringBYS.y_test))] \n",
    "dt_probs = bys.predict_proba(scoringBYS.X_test) \n",
    "dt_probs = dt_probs[:, 1]\n",
    "\n",
    "r_auc = roc_auc_score(scoringBYS.y_test , r_probs)\n",
    "dt_auc = roc_auc_score(scoringBYS.y_test , dt_probs)\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(scoringBYS.y_test, r_probs)\n",
    "dt_fpr , dt_tpr , _ = roc_curve(scoringBYS.y_test,dt_probs)\n",
    "\n",
    "plt.plot(r_fpr , r_tpr, linestyle = '--', label='Predicción aleatoria'% r_auc)\n",
    "plt.plot(dt_fpr , dt_tpr, marker='.', label='Naive Bayes'% dt_auc)\n",
    "\n",
    "plt.title('Gráfica ROC')\n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogitBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logitboost import LogitBoost as lgbt\n",
    "lboost = lgbt(n_estimators=100, random_state=0)\n",
    "scoringLboost = Analisis_Predictivo(bd_predictiva, predecir=\"Status\", modelo=lboost,\n",
    "                                       train_size=0.8, random_state=0)\n",
    "scoringLboost.distribucion_variable_predecir()\n",
    "plt.show()\n",
    "resboost = scoringLboost.fit_predict_resultados()\n",
    "sns.heatmap(resboost, annot=True, fmt='3.0f', xticklabels=('Falsos Positivos','Verdaderos Positivos'), \n",
    "            yticklabels=('Verdaderos Positivos','Falsos Positivos'), cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Matriz de Confusion (LogitBoost)', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear modelo\n",
    "# 100 arbolitos en el bosque\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "scoringrf = Analisis_Predictivo(bd_predictiva, predecir=\"Status\", modelo=rf,\n",
    "                                       train_size=0.8, random_state=0)\n",
    "scoringrf.distribucion_variable_predecir()\n",
    "plt.show()\n",
    "resrf = scoringrf.fit_predict_resultados()\n",
    "sns.heatmap(resrf, annot=True, fmt='3.0f', xticklabels=('Falsos Positivos','Verdaderos Positivos'), \n",
    "            yticklabels=('Verdaderos Positivos','Falsos Positivos'), cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Matriz de Confusion (Random Forest)', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "rf.fit(scoringrf.X_train, scoringrf.y_train)\n",
    "\n",
    "r_probs = [0 for _ in range(len(scoringrf.y_test))] \n",
    "dt_probs = rf.predict_proba(scoringrf.X_test) \n",
    "dt_probs = dt_probs[:, 1]\n",
    "\n",
    "r_auc = roc_auc_score(scoringrf.y_test , r_probs)\n",
    "dt_auc = roc_auc_score(scoringrf.y_test , dt_probs)\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(scoringrf.y_test, r_probs)\n",
    "dt_fpr , dt_tpr , _ = roc_curve(scoringrf.y_test,dt_probs)\n",
    "\n",
    "plt.plot(r_fpr , r_tpr, linestyle = '--', label='Predicción aleatoria'% r_auc)\n",
    "plt.plot(dt_fpr , dt_tpr, marker='.', label='Random Forest'% dt_auc)\n",
    "\n",
    "plt.title('Gráfica ROC')\n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(dt_probs, bins=20, kde=True, color='blue')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Histograma de Predicciones del Modelo Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que pasa si cambiamos la proporcion de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain=bd_predictiva.sample(frac=.8)\n",
    "dftest=bd_predictiva.drop(dftrain.index)\n",
    "dftrain=pd.concat((dftrain,dftrain[dftrain['Status']==1]))\n",
    "dftrain=pd.concat((dftrain,dftrain[dftrain['Status']==1]))\n",
    "plt.hist(dftrain['Status'],bins=3)\n",
    "plt.title('Proporcion de Pagadores y no pagadores conjunto de entrenamiento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dftest['Status'],bins=3)\n",
    "plt.title('Proporcion de Pagadores y no pagadores en el conjunto de validacion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogitBoost 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logitboost import LogitBoost as lgbt\n",
    "lboost = lgbt(n_estimators=100, random_state=0)\n",
    "scoringLboost = Analisis_Predictivo(dftrain, predecir=\"Status\", modelo=lboost,\n",
    "                                       train_size=0.7, random_state=1)\n",
    "scoringLboost.distribucion_variable_predecir()\n",
    "plt.show()\n",
    "resboost = scoringLboost.fit_predict_resultados()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resboost = scoringLboost.predictNew(dftest)\n",
    "sns.heatmap(resboost, annot=True, fmt='3.0f', xticklabels=('Falsos Positivos','Verdaderos Positivos'), \n",
    "            yticklabels=('Verdaderos Positivos','Falsos Positivos'), cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Matriz de Confusion LogitBoost (Datos duplicados)', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear modelo\n",
    "# 100 arbolitos en el bosque\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf2 = RandomForestClassifier(n_estimators=120,criterion='gini',max_depth=12)\n",
    "\n",
    "scoringrf2 = Analisis_Predictivo(dftrain, predecir=\"Status\", modelo=rf2,\n",
    "                                       train_size=0.3, random_state=42)\n",
    "scoringrf2.distribucion_variable_predecir()\n",
    "plt.show()\n",
    "resrf2 = scoringrf2.fit_predict_resultados()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resrf2 = scoringrf2.predictNew(dftest)\n",
    "sns.heatmap(resrf, annot=True, fmt='3.0f', xticklabels=('Falsos Positivos','Verdaderos Positivos'), \n",
    "            yticklabels=('Verdaderos Positivos','Falsos Positivos'), cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Matriz de Confusion Random Forest (Datos duplicados)', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "rf2.fit(scoringrf2.X_train, scoringrf2.y_train)\n",
    "\n",
    "r_probs = [0 for _ in range(len(scoringrf2.y_test))] \n",
    "dt_probs = rf.predict_proba(scoringrf2.X_test) \n",
    "dt_probs = dt_probs[:, 1]\n",
    "\n",
    "r_auc = roc_auc_score(scoringrf2.y_test , r_probs)\n",
    "dt_auc = roc_auc_score(scoringrf2.y_test , dt_probs)\n",
    "\n",
    "r_fpr, r_tpr, _ = roc_curve(scoringrf2.y_test, r_probs)\n",
    "dt_fpr , dt_tpr , _ = roc_curve(scoringrf2.y_test,dt_probs)\n",
    "\n",
    "plt.plot(r_fpr , r_tpr, linestyle = '--', label='Predicción aleatoria'% r_auc)\n",
    "plt.plot(dt_fpr , dt_tpr, marker='.', label='Random Forest 2'% dt_auc)\n",
    "\n",
    "plt.title('Gráfica ROC')\n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(dt_probs, bins=20, kde=True, color='blue')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Histograma de Predicciones del Modelo Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "for i,j in zip(rf2.feature_importances_,rf2.feature_names_in_):\n",
    "    print(f'{i:.4f}',j)\n",
    "    dict[j]=i\n",
    "\n",
    "from collections import OrderedDict\n",
    "od=OrderedDict(sorted(dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dftrain.columns:\n",
    "    plt.suptitle(col)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title('Pagadores')\n",
    "    plt.hist(dftrain[dftrain['Status']==1][col])\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('No Pagadores')\n",
    "    plt.hist(dftrain[dftrain['Status']==0][col])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
